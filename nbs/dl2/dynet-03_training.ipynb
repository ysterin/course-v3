{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "dynet_config.set(autobatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastprogress import progress_bar,  master_bar\n",
    "from fastai_dynet.models import *\n",
    "import PIL\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_01 import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return x_train,y_train,x_valid,y_valid\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nGOsZ8ANmPDIUSDJgmD+enWN/cVnE31GTMl3++/8YGH6zbyRTwlTchujHqesyA/W5aexSPryMzAwMPAzsNz5yY7hIH4I9eSVOhsu1/7ayOnFiEPy2/J3oVxYvcLw5eKVF8w3XupyYpF8sPEDA8PfEwznMrBIsknoC2gwMFzeedAeU1IqnIGBgYHB9M4JmCSWsDXE5RUGBgYGhr/fcEt+Y+bCLbkXw9h/u/5CGVu/mcMkYa59dPyNuzADA8P7zffFzdAlpfhuv3bmevTp6i+JaHgAMsJS38t1rxgYGBgYlFwkGdB1MoiHXD3JKqmtxobkRMYBSLcAG6REv2ssfO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC3B5FF00B8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.fromarray((127*(1.0-x_train.reshape(-1, 28, 28)[50])).astype(np.uint8), mode='L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset:\n",
    "    def __init__(self, xs, ys, shuffle=True):\n",
    "        assert len(xs) == len(ys)\n",
    "        self.xs, self.ys = xs, ys\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    def __getitem__(self, i):\n",
    "        return self.xs[i], self.ys[i]\n",
    "    def __iter__(self):\n",
    "        if self.shuffle: idxs = np.random.permutation(len(self))\n",
    "        else:       idxs = list(range(len(self)))\n",
    "        for i in range(len(self)):\n",
    "            yield self[idxs[i]]\n",
    "    def iter_batches(self, bs=64, shuffle=True):\n",
    "        if shuffle: idxs = np.random.permutation(len(self))\n",
    "        else:       idxs = list(range(len(self)))\n",
    "        batch_start = 0\n",
    "        while batch_start<len(self):\n",
    "            yield self[batch_start:batch_start+bs]\n",
    "            batch_start += bs\n",
    "            \n",
    "    def dataloader(self, bs=64, shuffle=True):\n",
    "        return Dataloader(self, bs, shuffle)\n",
    "            \n",
    "    \n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, dataset, bs=64, shuffle=True):\n",
    "        self.ds = dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ds)/self.bs))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle: idxs = np.random.permutation(len(self.ds))\n",
    "        else:       idxs = list(range(len(self.ds)))\n",
    "        batch_start = 0\n",
    "        while batch_start<len(self.ds):\n",
    "            bx, by = self.ds[idxs[batch_start:batch_start+self.bs]]\n",
    "            yield list(bx), by\n",
    "            batch_start += self.bs\n",
    "        \n",
    "class Databunch:\n",
    "    def __init__(self, train_ds, valid_ds, test_ds=None, bs=64):\n",
    "        self.train_ds, self.valid_ds, self.test_ds = train_ds, valid_ds, test_ds\n",
    "        self.train_dl = self.train_ds.dataloader(bs, shuffle=True)\n",
    "        self.valid_dl = self.valid_ds.dataloader(bs, shuffle=False)\n",
    "        if self.test_ds:\n",
    "            self.test_dl = self.test_ds.iter_batches(bs, shuffle=False)\n",
    "            \n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *layer_gens, parent=None, name=None, **kwargs):\n",
    "        super().__init__(parent, **kwargs)\n",
    "        self.layers = [l(parent=self) if l.__qualname__=='Module.create.<locals>.init' else l for l in layer_gens ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "\n",
    "class Input(Module):\n",
    "    def __init__(self, parent): super.__init__(parent)\n",
    "    def __call__(self, x):      return dy.inputVector(x)\n",
    "    \n",
    "class SimpleModel(Module):\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super(cls, SimpleModel).__new__(cls, is_top=True, **kwargs)\n",
    "    def __init__(self, n_in, n_hid, n_out, parent=None, **kwargs):\n",
    "        super().__init__(parent)\n",
    "        self.lin1 = Linear(n_in, n_hid)\n",
    "        Linear(n_hid, n_out, activ=None, parent=self, name='lin2')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = dy.inputTensor(x)\n",
    "        return self.lin2(self.lin1(x))\n",
    "\n",
    "def flatten(x):\n",
    "    bs = x.dim()[-1]\n",
    "    n_elem = np.prod(x.dim()[0])\n",
    "    return dy.reshape(x, (n_elem,), batch_size=bs)\n",
    "\n",
    "def avg_pool_2d(x):\n",
    "    return dy.mean_dim(x, [0,1], False)\n",
    "\n",
    "class Lambda(Module):\n",
    "    def __init__(self, func, parent=None, name=None):\n",
    "        name = name or func.__name__\n",
    "        super().__init__(self, parent, name)\n",
    "        self.func = func\n",
    "        \n",
    "    def __call__(x):\n",
    "        return self.func(x)\n",
    "\n",
    "class BatchNorm(Module):\n",
    "    def __init__(self, nc, parent=None, name=None):\n",
    "        super().__init__(parent, name)\n",
    "        self.nc = nc\n",
    "        self.mults = self.params.add_parameters((1, 1, nc), init=1)\n",
    "        self.adds = self.params.add_parameters((1, 1, nc), init=0)\n",
    "        self.mean = EWMA(init=np.zeros((1, 1, nc, 1)))\n",
    "        self.std = EWMA(init=np.zeros((1, 1, nc, 1)))\n",
    "\n",
    "        \n",
    "    def update_stats(self, x):\n",
    "        m = dy.reshape(dy.mean_dim(x, [0,1], True), (1, 1, self.nc))\n",
    "        s = dy.reshape(dy.std_dim(x, [0,1], True), (1, 1, self.nc))\n",
    "        if np.random.binomial(1, 0.05):\n",
    "            #dy.forward([m, s])\n",
    "            self.mean.update(m.npvalue())\n",
    "            self.std.update(s.npvalue())\n",
    "        return m, s\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self._training:\n",
    "            mean, std = self.update_stats(x) \n",
    "        else:\n",
    "            mean, std = dy.inputTensor(self.mean.value), dy.inputTensor(self.std.value)\n",
    "        return dy.cmult(dy.cdiv(x - mean, std), self.mults) + self.adds\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"batch normalization layer: number of channels={self.nc}\"\n",
    "    \n",
    "class LayerNorm(Module):\n",
    "    def __init__(self, nc, eps=1e-5, parent=None, name=None):\n",
    "        super().__init__(parent, name)\n",
    "        self.nc = nc\n",
    "        self.eps = eps\n",
    "        self.mults = self.params.add_parameters((1, 1, nc), init=1)\n",
    "        self.adds = self.params.add_parameters((1, 1, nc), init=0)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        mean = dy.reshape(dy.mean_dim(x, [0,1], False), (1, 1, self.nc))\n",
    "        std = dy.reshape(dy.std_dim(x, [0,1], False), (1, 1, self.nc))\n",
    "        return dy.cmult(dy.cdiv(x - mean, std + self.eps), self.mults) + self.adds\n",
    "        \n",
    "    \n",
    "class Conv2d(Module):\n",
    "    def __init__(self, c_in, c_out, ksize, stride=1, parent=None, name=None):\n",
    "        super().__init__(parent, name)\n",
    "        self.stride = stride\n",
    "        self.kernel = self.params.add_parameters((ksize, ksize, c_in, c_out), init='he', name='kernel')\n",
    "        self.bias = self.params.add_parameters(c_out, init=0, name='bias')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return dy.conv2d_bias(x, self.kernel, self.bias, (self.stride, self.stride))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b63276a36a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                is_top=True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minibatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Learner' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds = Dataset(x_train.reshape(-1, 28, 28), y_train), Dataset(x_valid.reshape(-1, 28, 28), y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)\n",
    "x = train_ds.xs[0]\n",
    "bx, by = next(iter(data.train_dl))\n",
    "m = Sequential(dy.inputTensor, Conv2d(1, 8, 5, stride=2), dy.rectify, Conv2d(8, 16, 3, stride=1), dy.rectify,\\\n",
    "               Conv2d(16, 32, 3, stride=2), dy.rectify, Conv2d(32, 32, 3, stride=2), \\\n",
    "               avg_pool_2d, Linear(32, 10),\\\n",
    "               is_top=True)\n",
    "\n",
    "learn = Learner(m, opt, data, batching='minibatch')\n",
    "loss = learn.loss_batch(bx, by)\n",
    "loss.backward()\n",
    "m.params.parameters_list()[-1].grad_as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = dy.SimpleSGDTrainer(m.params, learning_rate=4e-2)\n",
    "learn = Learner(m, opt, data, batching='autobatch', metrics=[accuracy])\n",
    "learn.callbacks[0].beta = 0.9\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = dy.SimpleSGDTrainer(m.params, learning_rate=4e-2)\n",
    "learn = Learner(m, opt, data, batching='minibatch', metrics=[accuracy])\n",
    "learn.callbacks[0].beta = 0.9\n",
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.101</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.141</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.094</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = dy.SimpleSGDTrainer(m.params, learning_rate=4e-1)\n",
    "learn = Learner(m, opt, data, batching='autobatch', metrics=[accuracy])\n",
    "learn.callbacks[0].beta = 0.9\n",
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.dynet_config.set_gpu()\n",
    "dy.dynet_config.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancelEpochException(Exception): pass\n",
    "class CancelTrainException(CancelEpochException): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, trainer, data, loss_func=None, batching='autobatch', cbfs=[], metrics=[]):\n",
    "        self.model,self.trainer,self.data,self.loss_func,self.batching = model,trainer,data,loss_func,batching\n",
    "        self.callbacks = [cbf(self) for cbf in cbfs]\n",
    "        self.callbacks.append(MetricsSummer(self, metrics=metrics))\n",
    "        self.callbacks.append(Recorder(self, metrics=metrics))\n",
    "        if not self.loss_func:\n",
    "            if batching == 'autobatch':\n",
    "                self.loss_func = dy.pickneglogsoftmax\n",
    "            elif batching == 'minibatch':\n",
    "                self.loss_func = dy.pickneglogsoftmax_batch\n",
    "        self._test = 1\n",
    "        \n",
    "    @property            \n",
    "    def output(self):\n",
    "        if self.batching == 'autobatch':\n",
    "            return [out.npvalue() for out in self._output]\n",
    "        elif self.batching == 'minibatch':\n",
    "            return np.rollaxis(self._output.npvalue(), axis=-1)\n",
    "    \n",
    "    @output.setter\n",
    "    def output(self, val):\n",
    "        self._output = val\n",
    "      \n",
    "    def loss_batch(self, bx, by):\n",
    "        dy.renew_cg()\n",
    "        if self.batching=='autobatch':\n",
    "            losses, outputs = [], []\n",
    "            for x, y in zip(bx, by):\n",
    "                out = self.model(x)\n",
    "                outputs.append(out)\n",
    "                losses.append(self.loss_func(out, y))\n",
    "            self.output = outputs\n",
    "            return dy.esum(losses)/len(losses)\n",
    "        elif self.batching=='minibatch':\n",
    "            out = self.model(bx)\n",
    "            self.output = out\n",
    "            return dy.sum_batches(self.loss_func(out, by))/len(by)\n",
    "        else: \n",
    "            raise \"invalid batching option\"\n",
    "        \n",
    "    def pred_batch(self, bx, by):\n",
    "        dy.renew_cg()\n",
    "        if self.batching=='autobatch':\n",
    "            outputs = []\n",
    "            for x, y in zip(bx, by):\n",
    "                outputs.append(self.model(x))\n",
    "            dy.forward(outputs)\n",
    "            return [out.npvalue() for out in outputs]\n",
    "        elif self.batching=='minibatch':\n",
    "            return np.rollaxis(self.model(bx).npvalue(), axis=-1)\n",
    "        \n",
    "    def one_batch(self, bx, by):\n",
    "        self(\"on_batch_begin\")\n",
    "        loss = self.loss_batch(bx, by); self(\"on_forward_bagin\")\n",
    "        loss.forward();                 self(\"on_backward_begin\", loss.value(), by)\n",
    "        if self.mode == 'eval':         return\n",
    "        loss.backward();                self(\"on_backward_end\")  \n",
    "        self.trainer.update();               self(\"on_batch_end\")\n",
    "        \n",
    "    def all_batches(self, dl):\n",
    "        self(\"on_epoch_begin\")\n",
    "        for bx, by in progress_bar(dl, parent=self.mbar):\n",
    "            try:\n",
    "                self.one_batch(bx, by)\n",
    "            except CancelEpochException:\n",
    "                break\n",
    "        self(\"on_epoch_end\")\n",
    "            \n",
    "    def fit(self, n_epochs):\n",
    "        self.mbar = master_bar(range(n_epochs))\n",
    "        self(\"on_train_begin\")\n",
    "        for epoch in self.mbar:\n",
    "            self.epoch = epoch\n",
    "            try:\n",
    "                self.train()\n",
    "                self.evaluate()\n",
    "            except CancelTrainException:\n",
    "                break\n",
    "            \n",
    "    def evaluate(self):\n",
    "        self.mode = 'eval'\n",
    "        self.all_batches(self.data.valid_dl)\n",
    "        \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "        self.all_batches(self.data.train_dl)\n",
    "            \n",
    "    def __call__(self, event, *args, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb(event, *args, **kwargs)\n",
    "            \n",
    "    def __getattr__(self, attr):\n",
    "        for cb in self.callbacks:\n",
    "            if cb.name() == attr:\n",
    "                return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWMA:\n",
    "    def __init__(self, beta=0.9):\n",
    "        self.beta = 0.9\n",
    "        self._value, self.n = 0, 0\n",
    "        \n",
    "    def update(self, val):\n",
    "        self.n += 1\n",
    "        self._value = (self.beta*self._value + (1-self.beta)*val)\n",
    "        return self.value\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value/(1-self.beta**self.n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fastprogress import master_bar, progress_bar\n",
    "class Callback:\n",
    "    _order = 0\n",
    "    def __init__(self, learn):\n",
    "        self.learn = learn\n",
    "     \n",
    "    @classmethod\n",
    "    def create(cls, *args, **kwargs):\n",
    "        def init(learn):\n",
    "            return cls(learn=learn, *args, **kwargs)\n",
    "        return init\n",
    "    \n",
    "    def __call__(self, event, *args, **kwargs):\n",
    "        f = getattr(self, event, None)\n",
    "        if f: return f(*args, **kwargs)\n",
    "    \n",
    "    @classmethod \n",
    "    def name(cls):\n",
    "        return cls.__name__.lower()\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.mb = master_bar()\n",
    "    \n",
    "    def on_epoch_begin(self):\n",
    "        pass\n",
    "\n",
    "def accuracy(inp, targ):\n",
    "    max_idxs = np.argmax(inp, axis=-1)\n",
    "    return np.sum(np.equal(max_idxs, targ))/targ.shape[0]\n",
    "\n",
    "class MetricsSummer(Callback):\n",
    "    def __init__(self, learn, metrics=[], beta=0.9):\n",
    "        super().__init__(learn)\n",
    "        self.metrics = metrics\n",
    "        self.beta = beta\n",
    "        #self.totals = defaultdict(int)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.learn.mbar.write(['loss'] + [met.__name__ for met in self.metrics], table=True)\n",
    "        \n",
    "    def on_epoch_begin(self):\n",
    "        self.totals = defaultdict(int)\n",
    "        self.smooth = EWMA(self.beta)\n",
    "        self.n_data = 0\n",
    "        \n",
    "    def on_backward_begin(self, loss, by):\n",
    "        self.learn.mbar.child.comment = f\"{self.smooth.update(loss): 0.4f}\"\n",
    "        self.n_data += len(by)\n",
    "        self.totals['loss'] += loss*len(by)\n",
    "        if self.learn.mode == 'train': return\n",
    "        for met in self.metrics:\n",
    "            self.totals[met.__name__] += met(self.learn.output, by)*len(by)\n",
    "        #raise CancelTrainException\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        if self.learn.mode=='eval':\n",
    "            self.learn.mbar.write([f'{v/self.n_data:0.3f}' for k, v in self.totals.items()], table=True)\n",
    "    \n",
    "class Recorder(Callback):\n",
    "    def __init__(self, learn, metrics, beta=0.9):\n",
    "        super().__init__(learn)\n",
    "        self.metrics = metrics\n",
    "        self.beta = beta\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.records = defaultdict(list)\n",
    "        \n",
    "    def on_epoch_begin(self):\n",
    "        self.avgs = defaultdict(lambda: EWMA(self.beta))\n",
    "        \n",
    "    def on_backward_begin(self, loss, by):\n",
    "        self.records['loss'].append(self.avgs['loss'].update(loss))\n",
    "        for met in self.metrics:\n",
    "            self.records[met.__name__].append(self.avgs[met.__name__].update(met(self.learn.output, by)))\n",
    "            \n",
    "    def plot(self):\n",
    "        for met, vals in self.records.items():\n",
    "            plt.ylabel(met)\n",
    "            plt.plot(vals, label=met)\n",
    "            plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer: in=784, out=128, activation=no_op\n",
      "Linear layer: in=128, out=10, activation=no_op\n"
     ]
    }
   ],
   "source": [
    "m = Sequential(dy.inputTensor, Linear(28**2, 128, activ=None), dy.rectify, Linear(128, 10, activ=None) , is_top=True)\n",
    "print(m)\n",
    "trainer = dy.SimpleSGDTrainer(m.params)\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)\n",
    "\n",
    "learn = Learner(m, trainer, data, batching='minibatch', metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.beta = 0.98\n",
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learn.recorder.records['loss'])\n",
    "plt.plot([0.41, 0.14, 0.41, 0.13, 0.51], [1000, 2000, 3000, 4000, 4500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, by = next(iter(learn.data.train_dl))\n",
    "loss = learn.loss_batch(bx, by)\n",
    "loss.forward()\n",
    "loss.backward()\n",
    "m.params.parameters_list()[2].grad_as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(learn.data.train_dl))\n",
    "dy.renew_cg()\n",
    "x = dy.inputTensor(x.T)\n",
    "x.dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dy.renew_cg()\n",
    "print(dy.inputTensor(x_train[:10], batched=True).dim())\n",
    "print(dy.inputTensor([x_train[i] for i in range(10)]).dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.inputTensor(train_ds[0][0]).dim(), dy.inputTensor(train_ds[:1][0], batched=True).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, loss_func, bs=64):\n",
    "    loss_sum, n_cor = 0, 0\n",
    "    for bx, by in ds.iter_batches(bs):\n",
    "        losses, logits = [], []\n",
    "        dy.renew_cg()\n",
    "        for x, y in zip(bx, by):\n",
    "            out = m(x)\n",
    "            losses.append(loss_func(out, y))\n",
    "            logits.append(out)\n",
    "        dy.forward(logits)\n",
    "        loss_sum += dy.esum(losses).value()\n",
    "        n_cor += sum(np.argmax(log.npvalue()) == y for log, y in zip(logits, by))\n",
    "    return loss_sum/len(ds), n_cor/len(ds)\n",
    "\n",
    "evaluate(m, valid_ds, dy.pickneglogsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22 seconds\n",
    "m = Model1(28**2, 128,  10)\n",
    "n_epochs = 3\n",
    "bs = 64\n",
    "trainer = dy.SimpleSGDTrainer(m.params, learning_rate=0.1)\n",
    "loss_func = dy.pickneglogsoftmax\n",
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        for bx, by in train_ds.iter_batches(bs):\n",
    "            loss = m.loss_batch(bx, by, dy.pickneglogsoftmax_batch, batching='minibatch')\n",
    "            #print(loss.value())\n",
    "            loss.forward()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        avg_loss, acc = evaluate(m, valid_ds, loss_func=dy.pickneglogsoftmax)\n",
    "        print(epoch, f' loss: {avg_loss}, accuracy: {acc}')\n",
    "                \n",
    "fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 seconds\n",
    "m = Model1(28**2, 128,  10)\n",
    "n_epochs = 3\n",
    "bs = 64\n",
    "trainer = dy.SimpleSGDTrainer(m.params, learning_rate=0.1)\n",
    "loss_func = dy.pickneglogsoftmax\n",
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        for bx, by in train_ds.iter_batches(bs):\n",
    "            loss = m.loss_batch(bx, by, dy.pickneglogsoftmax, batching='autobatch')\n",
    "            loss.forward()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        avg_loss, acc = evaluate(m, valid_ds, loss_func=dy.pickneglogsoftmax)\n",
    "        print(epoch, f' loss: {avg_loss}, accuracy: {acc}')\n",
    "                \n",
    "fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.renew_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, by = train_ds[:10]\n",
    "list(zip(bx, by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.learning_rate = 0.1\n",
    "?trainer.restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
