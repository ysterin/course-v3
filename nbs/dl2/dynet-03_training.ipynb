{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "dynet_config.set(autobatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from fastprogress import progress_bar,  master_bar\n",
    "from fastai_dynet import models\n",
    "import PIL\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_01 import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return x_train,y_train,x_valid,y_valid\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nGOsZ8ANmPDIUSDJgmD+enWN/cVnE31GTMl3++/8YGH6zbyRTwlTchujHqesyA/W5aexSPryMzAwMPAzsNz5yY7hIH4I9eSVOhsu1/7ayOnFiEPy2/J3oVxYvcLw5eKVF8w3XupyYpF8sPEDA8PfEwznMrBIsknoC2gwMFzeedAeU1IqnIGBgYHB9M4JmCSWsDXE5RUGBgYGhr/fcEt+Y+bCLbkXw9h/u/5CGVu/mcMkYa59dPyNuzADA8P7zffFzdAlpfhuv3bmevTp6i+JaHgAMsJS38t1rxgYGBgYlFwkGdB1MoiHXD3JKqmtxobkRMYBSLcAG6REv2ssfO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F33B539D550>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.fromarray((127*(1.0-x_train.reshape(-1, 28, 28)[50])).astype(np.uint8), mode='L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset:\n",
    "    def __init__(self, xs, ys, shuffle=True):\n",
    "        assert len(xs) == len(ys)\n",
    "        self.xs, self.ys = xs, ys\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    def __getitem__(self, i):\n",
    "        return self.xs[i], self.ys[i]\n",
    "    def __iter__(self):\n",
    "        if self.shuffle: idxs = np.random.permutation(len(self))\n",
    "        else:       idxs = list(range(len(self)))\n",
    "        for i in range(len(self)):\n",
    "            yield self[idxs[i]]\n",
    "    def iter_batches(self, bs=64, shuffle=True):\n",
    "        if shuffle: idxs = np.random.permutation(len(self))\n",
    "        else:       idxs = list(range(len(self)))\n",
    "        batch_start = 0\n",
    "        while batch_start<len(self):\n",
    "            yield self[batch_start:batch_start+bs]\n",
    "            batch_start += bs\n",
    "            \n",
    "    def dataloader(self, bs=64, shuffle=True):\n",
    "        return Dataloader(self, bs, shuffle)\n",
    "            \n",
    "    \n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, dataset, bs=64, shuffle=True):\n",
    "        self.ds = dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ds)/self.bs))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle: idxs = np.random.permutation(len(self.ds))\n",
    "        else:       idxs = list(range(len(self.ds)))\n",
    "        batch_start = 0\n",
    "        while batch_start<len(self.ds):\n",
    "            bx, by = self.ds[idxs[batch_start:batch_start+self.bs]]\n",
    "            yield list(bx), by\n",
    "            batch_start += self.bs\n",
    "        \n",
    "class Databunch:\n",
    "    def __init__(self, train_ds, valid_ds, test_ds=None, bs=64):\n",
    "        self.train_ds, self.valid_ds, self.test_ds = train_ds, valid_ds, test_ds\n",
    "        self.train_dl = self.train_ds.dataloader(bs, shuffle=True)\n",
    "        self.valid_dl = self.valid_ds.dataloader(bs, shuffle=False)\n",
    "        if self.test_ds:\n",
    "            self.test_dl = self.test_ds.iter_batches(bs, shuffle=False)\n",
    "            \n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "from fastai_dynet.models import *\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *layer_gens, parent=None, name=None, **kwargs):\n",
    "        super().__init__(parent, **kwargs)\n",
    "        self.layers = [l(parent=self) if l.__qualname__=='Module.create.<locals>.init' else l for l in layer_gens ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "\n",
    "class Input(Module):\n",
    "    def __init__(self, parent): super.__init__(parent)\n",
    "    def __call__(self, x):      return dy.inputVector(x)\n",
    "    \n",
    "class SimpleModel(Module):\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super(cls, SimpleModel).__new__(cls, is_top=True, **kwargs)\n",
    "    def __init__(self, n_in, n_hid, n_out, parent=None, **kwargs):\n",
    "        super().__init__(parent)\n",
    "        self.lin1 = Linear(n_in, n_hid)\n",
    "        Linear(n_hid, n_out, activ=None, parent=self, name='lin2')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = dy.inputTensor(x)\n",
    "        return self.lin2(self.lin1(x))\n",
    "\n",
    "def flatten(x):\n",
    "    bs = x.dim()[-1]\n",
    "    n_elem = np.prod(x.dim()[0])\n",
    "    return dy.reshape(x, (n_elem,), batch_size=bs)\n",
    "    \n",
    "class Conv2d(Module):\n",
    "    def __init__(self, c_in, c_out, ksize, stride=1, parent=None, name=None):\n",
    "        super().__init__(parent, name)\n",
    "        self.kernel = self.params.add_parameters((ksize, ksize, c_in, c_out), init='he', name='kernel')\n",
    "        self.bias = self.params.add_parameters(c_out, init=0, name='bias')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return dy.conv2d_bias(x, self.kernel, self.bias, self.stride)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28), 64)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = Dataset(x_train.reshape(-1, 28, 28), y_train), Dataset(x_valid.reshape(-1, 28, 28), y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)\n",
    "x = train_ds.xs[0]\n",
    "x = dy.inputTensor(x)\n",
    "bx, by = next(iter(data.train_dl))\n",
    "dy.inputTensor(bx).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancelEpochException(Exception): pass\n",
    "class CancelTrainException(CancelEpochException): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, trainer, data, loss_func=None, batching='autobatch', cbfs=[], metrics=[]):\n",
    "        self.model,self.trainer,self.data,self.loss_func,self.batching = model,trainer,data,loss_func,batching\n",
    "        self.callbacks = [cbf(self) for cbf in cbfs]\n",
    "        self.callbacks.append(MetricsSummer(self, metrics=metrics))\n",
    "        if not self.loss_func:\n",
    "            if batching == 'autobatch':\n",
    "                self.loss_func = dy.pickneglogsoftmax\n",
    "            elif batching == 'minibatch':\n",
    "                self.loss_func = dy.pickneglogsoftmax_batch\n",
    "        self._test = 1\n",
    "        \n",
    "    @property            \n",
    "    def output(self):\n",
    "        if self.batching == 'autobatch':\n",
    "            return [out.npvalue() for out in self._output]\n",
    "        elif self.batching == 'minibatch':\n",
    "            return np.rollaxis(self._output.npvalue(), axis=-1)\n",
    "    \n",
    "    @output.setter\n",
    "    def output(self, val):\n",
    "        self._output = val\n",
    "      \n",
    "    def loss_batch(self, bx, by):\n",
    "        dy.renew_cg()\n",
    "        if self.batching=='autobatch':\n",
    "            losses, outputs = [], []\n",
    "            for x, y in zip(bx, by):\n",
    "                out = self.model(x)\n",
    "                outputs.append(out)\n",
    "                losses.append(self.loss_func(out, y))\n",
    "            self.output = outputs\n",
    "            return dy.esum(losses)/len(losses)\n",
    "        elif self.batching=='minibatch':\n",
    "            out = self.model(bx)\n",
    "            self.output = out\n",
    "            return dy.sum_batches(self.loss_func(out, by))/len(by)\n",
    "        else: \n",
    "            raise \"invalid batching option\"\n",
    "        \n",
    "    def pred_batch(self, bx, by):\n",
    "        dy.renew_cg()\n",
    "        if self.batching=='autobatch':\n",
    "            outputs = []\n",
    "            for x, y in zip(bx, by):\n",
    "                outputs.append(self.model(x))\n",
    "            dy.forward(outputs)\n",
    "            return [out.npvalue() for out in outputs]\n",
    "        elif self.batching=='minibatch':\n",
    "            return np.rollaxis(self.model(bx).npvalue(), axis=-1)\n",
    "        \n",
    "    def one_batch(self, bx, by):\n",
    "        self(\"on_batch_begin\")\n",
    "        loss = self.loss_batch(bx, by); self(\"on_forward_bagin\")\n",
    "        loss.forward();                 self(\"on_backward_begin\", loss.value(), by)\n",
    "        if self.mode == 'eval':         return\n",
    "        loss.backward();                self(\"on_backward_end\")  \n",
    "        trainer.update();               self(\"on_batch_end\")\n",
    "        \n",
    "    def all_batches(self, dl):\n",
    "        self(\"on_epoch_begin\")\n",
    "        for bx, by in progress_bar(dl, parent=self.mbar):\n",
    "            try:\n",
    "                self.one_batch(bx, by)\n",
    "            except CancelEpochException:\n",
    "                break\n",
    "        self(\"on_epoch_end\")\n",
    "            \n",
    "    def fit(self, n_epochs):\n",
    "        self.mbar = master_bar(range(n_epochs))\n",
    "        self(\"on_train_begin\")\n",
    "        for epoch in self.mbar:\n",
    "            self.epoch = epoch\n",
    "            try:\n",
    "                self.train()\n",
    "                self.evaluate()\n",
    "            except CancelTrainException:\n",
    "                break\n",
    "            \n",
    "    def evaluate(self):\n",
    "        self.mode = 'eval'\n",
    "        self.all_batches(self.data.valid_dl)\n",
    "        \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "        self.all_batches(self.data.train_dl)\n",
    "            \n",
    "    def __call__(self, event, *args, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb(event, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fastprogress import master_bar, progress_bar\n",
    "class Callback:\n",
    "    _order = 0\n",
    "    def __init__(self, learn):\n",
    "        self.learn = learn\n",
    "     \n",
    "    @classmethod\n",
    "    def create(cls, *args, **kwargs):\n",
    "        def init(learn):\n",
    "            return cls(learn=learn, *args, **kwargs)\n",
    "        return init\n",
    "    \n",
    "    def __call__(self, event, *args, **kwargs):\n",
    "        f = getattr(self, event, None)\n",
    "        if f: return f(*args, **kwargs)\n",
    "        #return self.__getattr__(event)(*args, **kwargs)\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.mb = master_bar()\n",
    "    \n",
    "    def on_epoch_begin(self):\n",
    "        pass\n",
    "    \n",
    "class MetricsSummer(Callback):\n",
    "    def __init__(self, learn, metrics=[]):\n",
    "        super().__init__(learn)\n",
    "        self.metrics = metrics\n",
    "        #self.totals = defaultdict(int)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.learn.mbar.write(['loss'] + [met.__name__ for met in self.metrics], table=True)\n",
    "        \n",
    "    def on_epoch_begin(self):\n",
    "        self.totals = defaultdict(int)\n",
    "        self.n_data = 0\n",
    "        \n",
    "    def on_backward_begin(self, loss, by):\n",
    "        self.learn.mbar.child.comment = f\"{loss: 0.4f}\"\n",
    "        self.n_data += len(by)\n",
    "        self.totals['loss'] += loss*len(by)\n",
    "        if self.learn.mode == 'train': return\n",
    "        for met in self.metrics:\n",
    "            self.totals[met.__name__] += met(self.learn.output, by)*len(by)\n",
    "        #raise CancelTrainException\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        if self.learn.mode=='eval':\n",
    "            self.learn.mbar.write([f'{v/self.n_data:0.3f}' for k, v in self.totals.items()], table=True)\n",
    "    \n",
    "class Recorder(Callback):\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer: in=784, out=128, activation=no_op\n",
      "Linear layer: in=128, out=10, activation=no_op\n"
     ]
    }
   ],
   "source": [
    "m = Sequential(dy.inputTensor, Linear(28**2, 128, activ=None), dy.rectify, Linear(128, 10, activ=None) , is_top=True)\n",
    "print(m)\n",
    "trainer = dy.SimpleSGDTrainer(m.params)\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "data = Databunch(train_ds, valid_ds, bs=64)\n",
    "def accuracy(inp, targ):\n",
    "    #print(inp.shape, targ.shape)\n",
    "    max_idxs = np.argmax(inp, axis=-1)\n",
    "    return np.sum(np.equal(max_idxs, targ))/targ.shape[0]\n",
    "learn = Learner(m, trainer, data, batching='minibatch', metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.259</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.223</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.165</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.144</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.123</td>\n",
       "      <td>0.967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 64), 1)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(learn.data.train_dl))\n",
    "dy.renew_cg()\n",
    "x = dy.inputTensor(x.T)\n",
    "x.dim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10,), 784)\n",
      "((784,), 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dy.renew_cg()\n",
    "print(dy.inputTensor(x_train[:10], batched=True).dim())\n",
    "print(dy.inputTensor([x_train[i] for i in range(10)]).dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((784,), 1), ((784,), 1))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.inputTensor(train_ds[0][0]).dim(), dy.inputTensor(train_ds[:1][0], batched=True).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13049399704933168, 0.9662)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model, ds, loss_func, bs=64):\n",
    "    loss_sum, n_cor = 0, 0\n",
    "    for bx, by in ds.iter_batches(bs):\n",
    "        losses, logits = [], []\n",
    "        dy.renew_cg()\n",
    "        for x, y in zip(bx, by):\n",
    "            out = m(x)\n",
    "            losses.append(loss_func(out, y))\n",
    "            logits.append(out)\n",
    "        dy.forward(logits)\n",
    "        loss_sum += dy.esum(losses).value()\n",
    "        n_cor += sum(np.argmax(log.npvalue()) == y for log, y in zip(logits, by))\n",
    "    return loss_sum/len(ds), n_cor/len(ds)\n",
    "\n",
    "evaluate(m, valid_ds, dy.pickneglogsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  loss: 0.2745085589885712, accuracy: 0.9215\n",
      "1  loss: 0.20155801968574524, accuracy: 0.9457\n",
      "2  loss: 0.16082635686397553, accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "#22 seconds\n",
    "m = Model1(28**2, 128,  10)\n",
    "n_epochs = 3\n",
    "bs = 64\n",
    "trainer = dy.SimpleSGDTrainer(m.params, learning_rate=0.1)\n",
    "loss_func = dy.pickneglogsoftmax\n",
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        for bx, by in train_ds.iter_batches(bs):\n",
    "            loss = m.loss_batch(bx, by, dy.pickneglogsoftmax_batch, batching='minibatch')\n",
    "            #print(loss.value())\n",
    "            loss.forward()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        avg_loss, acc = evaluate(m, valid_ds, loss_func=dy.pickneglogsoftmax)\n",
    "        print(epoch, f' loss: {avg_loss}, accuracy: {acc}')\n",
    "                \n",
    "fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  loss: 0.27814263706207276, accuracy: 0.9213\n",
      "1  loss: 0.19909487006664275, accuracy: 0.9455\n",
      "2  loss: 0.15932156596183777, accuracy: 0.9565\n"
     ]
    }
   ],
   "source": [
    "#20 seconds\n",
    "m = Model1(28**2, 128,  10)\n",
    "n_epochs = 3\n",
    "bs = 64\n",
    "trainer = dy.SimpleSGDTrainer(m.params, learning_rate=0.1)\n",
    "loss_func = dy.pickneglogsoftmax\n",
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        for bx, by in train_ds.iter_batches(bs):\n",
    "            loss = m.loss_batch(bx, by, dy.pickneglogsoftmax, batching='autobatch')\n",
    "            loss.forward()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        avg_loss, acc = evaluate(m, valid_ds, loss_func=dy.pickneglogsoftmax)\n",
    "        print(epoch, f' loss: {avg_loss}, accuracy: {acc}')\n",
    "                \n",
    "fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.renew_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 5),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 0),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 4),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 1),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 9),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 2),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 1),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 3),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 1),\n",
       " (array([0., 0., 0., 0., ..., 0., 0., 0., 0.], dtype=float32), 4)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx, by = train_ds[:10]\n",
    "list(zip(bx, by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.learning_rate = 0.1\n",
    "?trainer.restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
